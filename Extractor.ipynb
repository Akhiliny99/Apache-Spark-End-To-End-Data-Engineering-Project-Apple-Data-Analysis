{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d627d1b1-e1ac-499d-a45c-b48e59eb3389",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----------+------------+----------------+\n|transaction_id|customer_id|product_name|transaction_date|\n+--------------+-----------+------------+----------------+\n|            11|        105|      iPhone|      2022-02-01|\n|            12|        106|      iPhone|      2022-02-02|\n|            13|        107|     AirPods|      2022-02-03|\n|            14|        105|     AirPods|      2022-02-04|\n|            15|        108|      iPhone|      2022-02-05|\n|            16|        106|     MacBook|      2022-02-06|\n|            17|        107|      iPhone|      2022-02-07|\n|            18|        105|     MacBook|      2022-02-08|\n|            19|        108|     AirPods|      2022-02-09|\n|            20|        106|     AirPods|      2022-02-10|\n+--------------+-----------+------------+----------------+\n\n+-----------+-------------+-------------------+--------+\n|customer_id|customer_name|          join_date|location|\n+-----------+-------------+-------------------+--------+\n|        105|          Eva|2022-01-01 00:00:00|    Ohio|\n|        106|        Frank|2022-02-01 00:00:00|  Nevada|\n|        107|        Grace|2022-03-01 00:00:00|Colorado|\n|        108|        Henry|2022-04-01 00:00:00|    Utah|\n+-----------+-------------+-------------------+--------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "class Extractor:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def extract(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "class AirpodsAfterIphone(Extractor):\n",
    "    def __init__(self):\n",
    "        self.spark = SparkSession.builder.appName(\"Extractor\").getOrCreate()\n",
    "\n",
    "    def extract(self):\n",
    "        # Extracting transaction data from CSV\n",
    "        transaction_input_df = self.spark.read.csv(\n",
    "            \"dbfs:/FileStore/tables/Transaction_Updated.csv\", header=True, inferSchema=True\n",
    "        )\n",
    "\n",
    "        # Extracting customer data from Delta Table\n",
    "        customer_input_df = self.spark.read.format(\"delta\").table(\"default.customer_delta_table\")\n",
    "\n",
    "        return transaction_input_df, customer_input_df\n",
    "\n",
    "\n",
    "# Run the extractor\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize the extractor and extract data\n",
    "    extractor = AirpodsAfterIphone()\n",
    "    try:\n",
    "        transaction_input_df, customer_input_df = extractor.extract()\n",
    "\n",
    "        # Check if data is extracted properly\n",
    "        if transaction_input_df is not None and customer_input_df is not None:\n",
    "            transaction_input_df.show()\n",
    "            customer_input_df.show()\n",
    "        else:\n",
    "            print(\"‚ùå No data extracted.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting data: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb4002ee-8b57-4ede-80a1-6a8cee87f8cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----------+------------+----------------+\n|transaction_id|customer_id|product_name|transaction_date|\n+--------------+-----------+------------+----------------+\n|            11|        105|      iPhone|      2022-02-01|\n|            12|        106|      iPhone|      2022-02-02|\n|            13|        107|     AirPods|      2022-02-03|\n|            14|        105|     AirPods|      2022-02-04|\n|            15|        108|      iPhone|      2022-02-05|\n|            16|        106|     MacBook|      2022-02-06|\n|            17|        107|      iPhone|      2022-02-07|\n|            18|        105|     MacBook|      2022-02-08|\n|            19|        108|     AirPods|      2022-02-09|\n|            20|        106|     AirPods|      2022-02-10|\n+--------------+-----------+------------+----------------+\n\n+-----------+-------------+-------------------+--------+\n|customer_id|customer_name|          join_date|location|\n+-----------+-------------+-------------------+--------+\n|        105|          Eva|2022-01-01 00:00:00|    Ohio|\n|        106|        Frank|2022-02-01 00:00:00|  Nevada|\n|        107|        Grace|2022-03-01 00:00:00|Colorado|\n|        108|        Henry|2022-04-01 00:00:00|    Utah|\n+-----------+-------------+-------------------+--------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "class Extractor:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def extract(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "class AirpodsAfterIphone(Extractor):\n",
    "    def __init__(self):\n",
    "        self.spark = SparkSession.builder.appName(\"Extractor\").getOrCreate()\n",
    "\n",
    "    def extract(self):\n",
    "        # Extracting transaction data from CSV\n",
    "        transaction_input_df = self.spark.read.csv(\n",
    "            \"dbfs:/FileStore/tables/Transaction_Updated.csv\", header=True, inferSchema=True\n",
    "        )\n",
    "\n",
    "        # Extracting customer data from Delta Table\n",
    "        customer_input_df = self.spark.read.format(\"delta\").table(\"default.customer_delta_table\")\n",
    "\n",
    "        return transaction_input_df, customer_input_df\n",
    "\n",
    "\n",
    "# Run the extractor\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize the extractor and extract data\n",
    "    extractor = AirpodsAfterIphone()\n",
    "    transaction_input_df, customer_input_df = extractor.extract()\n",
    "\n",
    "    # You can save the extracted data to files or just print them\n",
    "    transaction_input_df.show()\n",
    "    customer_input_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a202df69-e0ce-4a2e-a2b8-0f4e212c74ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----------+------------+----------------+\n|transaction_id|customer_id|product_name|transaction_date|\n+--------------+-----------+------------+----------------+\n|            11|        105|      iPhone|      2022-02-01|\n|            12|        106|      iPhone|      2022-02-02|\n|            13|        107|     AirPods|      2022-02-03|\n|            14|        105|     AirPods|      2022-02-04|\n|            15|        108|      iPhone|      2022-02-05|\n|            16|        106|     MacBook|      2022-02-06|\n|            17|        107|      iPhone|      2022-02-07|\n|            18|        105|     MacBook|      2022-02-08|\n|            19|        108|     AirPods|      2022-02-09|\n|            20|        106|     AirPods|      2022-02-10|\n+--------------+-----------+------------+----------------+\n\n+-----------+-------------+-------------------+--------+\n|customer_id|customer_name|          join_date|location|\n+-----------+-------------+-------------------+--------+\n|        105|          Eva|2022-01-01 00:00:00|    Ohio|\n|        106|        Frank|2022-02-01 00:00:00|  Nevada|\n|        107|        Grace|2022-03-01 00:00:00|Colorado|\n|        108|        Henry|2022-04-01 00:00:00|    Utah|\n+-----------+-------------+-------------------+--------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "class Extractor:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def extract(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "class AirpodsAfterIphone(Extractor):\n",
    "    def __init__(self):\n",
    "        self.spark = SparkSession.builder.appName(\"Extractor\").getOrCreate()\n",
    "\n",
    "    def extract(self):\n",
    "        # Extracting transaction data from CSV\n",
    "        transaction_input_df = self.spark.read.csv(\n",
    "            \"dbfs:/FileStore/tables/Transaction_Updated.csv\", header=True, inferSchema=True\n",
    "        )\n",
    "\n",
    "        # Extracting customer data from Delta Table\n",
    "        customer_input_df = self.spark.read.format(\"delta\").table(\"default.customer_delta_table\")\n",
    "\n",
    "        return transaction_input_df, customer_input_df\n",
    "\n",
    "\n",
    "# Run the extractor\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize the extractor and extract data\n",
    "    extractor = AirpodsAfterIphone()\n",
    "    transaction_input_df, customer_input_df = extractor.extract()\n",
    "\n",
    "    # You can save the extracted data to files or just print them\n",
    "    transaction_input_df.show()\n",
    "    customer_input_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "614041f1-2089-4c42-a0dd-3fe4e6c4a471",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----------+------------+----------------+\n|transaction_id|customer_id|product_name|transaction_date|\n+--------------+-----------+------------+----------------+\n|            11|        105|      iPhone|      2022-02-01|\n|            12|        106|      iPhone|      2022-02-02|\n|            13|        107|     AirPods|      2022-02-03|\n|            14|        105|     AirPods|      2022-02-04|\n|            15|        108|      iPhone|      2022-02-05|\n|            16|        106|     MacBook|      2022-02-06|\n|            17|        107|      iPhone|      2022-02-07|\n|            18|        105|     MacBook|      2022-02-08|\n|            19|        108|     AirPods|      2022-02-09|\n|            20|        106|     AirPods|      2022-02-10|\n+--------------+-----------+------------+----------------+\n\n+-----------+-------------+-------------------+--------+\n|customer_id|customer_name|          join_date|location|\n+-----------+-------------+-------------------+--------+\n|        105|          Eva|2022-01-01 00:00:00|    Ohio|\n|        106|        Frank|2022-02-01 00:00:00|  Nevada|\n|        107|        Grace|2022-03-01 00:00:00|Colorado|\n|        108|        Henry|2022-04-01 00:00:00|    Utah|\n+-----------+-------------+-------------------+--------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "class Extractor:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def extract(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "class AirpodsAfterIphone(Extractor):\n",
    "    def __init__(self):\n",
    "        self.spark = SparkSession.builder.appName(\"Extractor\").getOrCreate()\n",
    "\n",
    "    def extract(self):\n",
    "        # Extracting transaction data from CSV\n",
    "        transaction_input_df = self.spark.read.csv(\n",
    "            \"dbfs:/FileStore/tables/Transaction_Updated.csv\", header=True, inferSchema=True\n",
    "        )\n",
    "\n",
    "        # Extracting customer data from Delta Table\n",
    "        customer_input_df = self.spark.read.format(\"delta\").table(\"default.customer_delta_table\")\n",
    "\n",
    "        input_df = {\n",
    "            \"transaction_input_df\": transaction_input_df,\n",
    "            \"customer_input_df\": customer_input_df\n",
    "        }\n",
    "\n",
    "        return input_df\n",
    "\n",
    "\n",
    "# Run the extractor\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize the extractor and extract data\n",
    "    extractor = AirpodsAfterIphone()\n",
    "    extracted_data = extractor.extract()\n",
    "\n",
    "    # You can save the extracted data to files or just print them\n",
    "    extracted_data[\"transaction_input_df\"].show()\n",
    "    extracted_data[\"customer_input_df\"].show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "20de3cd7-77d4-4b03-a93c-6bcf1e8d7cfd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class Extractor:\n",
    "    def __init__(self):\n",
    "        self.spark = SparkSession.builder.appName(\"Extractor\").getOrCreate()\n",
    "\n",
    "    def extract(self):\n",
    "        print(\"üîÑ Extracting Data...\")\n",
    "\n",
    "        try:\n",
    "            # Load Transaction Data from CSV\n",
    "            transaction_input_df = self.spark.read.csv(\n",
    "                \"dbfs:/FileStore/tables/Transaction_Updated.csv\", header=True, inferSchema=True\n",
    "            ).orderBy(\"customer_id\", \"transaction_date\")\n",
    "            \n",
    "            # Check if the DataFrame is loaded successfully\n",
    "            if transaction_input_df.count() == 0:\n",
    "                print(\"‚ö†Ô∏è No data in transaction input.\")\n",
    "            else:\n",
    "                print(f\"‚úÖ Transaction Data Loaded: {transaction_input_df.count()} rows\")\n",
    "            \n",
    "            transaction_input_df.show(5)\n",
    "\n",
    "            # Load Customer Data from Delta Table\n",
    "            customer_input_df = self.spark.read.format(\"delta\").table(\"default.customer_delta_table\")\n",
    "\n",
    "            # Check if customer DataFrame has data\n",
    "            if customer_input_df.count() == 0:\n",
    "                print(\"‚ö†Ô∏è No data found in customer delta table.\")\n",
    "            else:\n",
    "                print(f\"‚úÖ Customer Data Loaded: {customer_input_df.count()} rows\")\n",
    "            \n",
    "            customer_input_df.show(5)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error loading data: {e}\")\n",
    "            return None  # Return None if there's an error\n",
    "\n",
    "        # ‚úÖ Return a dictionary of dataframes if everything is loaded\n",
    "        return {\n",
    "            \"transaction_input_df\": transaction_input_df,\n",
    "            \"customer_input_df\": customer_input_df\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a5de9028-33f8-49dd-958f-d43e39aa8a89",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class Extractor:\n",
    "    def __init__(self):\n",
    "        self.spark = SparkSession.builder.appName(\"Extractor\").getOrCreate()\n",
    "\n",
    "    def extract(self):\n",
    "        print(\"üîÑ Extracting Data...\")\n",
    "\n",
    "        try:\n",
    "            # Load Transaction Data from CSV\n",
    "            transaction_input_df = self.spark.read.csv(\n",
    "                \"dbfs:/FileStore/tables/Transaction_Updated.csv\", header=True, inferSchema=True\n",
    "            ).orderBy(\"customer_id\", \"transaction_date\")\n",
    "\n",
    "            print(f\"‚úÖ Transaction Data Loaded: {transaction_input_df.count()} rows\")\n",
    "            transaction_input_df.show(5)\n",
    "\n",
    "            # Load Customer Data from Delta Table\n",
    "            customer_input_df = self.spark.read.format(\"delta\").table(\"default.customer_delta_table\")\n",
    "\n",
    "            if customer_input_df.count() > 0:\n",
    "                print(f\"‚úÖ Customer Data Loaded: {customer_input_df.count()} rows\")\n",
    "                customer_input_df.show(5)\n",
    "            else:\n",
    "                print(\"‚ö†Ô∏è Warning: No data found in Delta table!\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error loading data: {e}\")\n",
    "            return None  # Return None if there's an error\n",
    "\n",
    "        # ‚úÖ Ensure the method returns a valid dictionary\n",
    "        return {\n",
    "            \"transaction_input_df\": transaction_input_df,\n",
    "            \"customer_input_df\": customer_input_df\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3d834d77-07f4-4a17-bb6a-2c33f98c9fb0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class Extractor:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def extract(self):\n",
    "        pass\n",
    "\n",
    "class AirpodsAfterIphone(Extractor):\n",
    "    def extract(self):\n",
    "        transaction_input_df = get_data_source(\n",
    "            data_type=\"csv\",\n",
    "            file_path=\"dbfs:/FileStore/tables/Transaction_Updated.csv\"\n",
    "        ).get_data_frame()\n",
    "        transaction_input_df.orderBy(\"customer_id\", \"transaction_date\").show()\n",
    "\n",
    "        customer_input_df = get_data_source(\n",
    "            data_type=\"delta\",\n",
    "            file_path=\"default.customer_delta_table\"\n",
    "        ).get_data_frame()\n",
    "\n",
    "        input_df = {\n",
    "            \"transaction_input_df\": transaction_input_df,\n",
    "            \"customer_input_df\": customer_input_df\n",
    "        }\n",
    "\n",
    "        return input_df\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Extractor",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
