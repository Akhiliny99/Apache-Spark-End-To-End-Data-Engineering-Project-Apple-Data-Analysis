{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92fb0168-bfb8-45c3-b15d-d75774c793a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----------+------------+----------------+\n|transaction_id|customer_id|product_name|transaction_date|\n+--------------+-----------+------------+----------------+\n|            11|        105|      iPhone|      2022-02-01|\n|            12|        106|      iPhone|      2022-02-02|\n|            13|        107|     AirPods|      2022-02-03|\n|            14|        105|     AirPods|      2022-02-04|\n|            15|        108|      iPhone|      2022-02-05|\n|            16|        106|     MacBook|      2022-02-06|\n|            17|        107|      iPhone|      2022-02-07|\n|            18|        105|     MacBook|      2022-02-08|\n|            19|        108|     AirPods|      2022-02-09|\n|            20|        106|     AirPods|      2022-02-10|\n+--------------+-----------+------------+----------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "spark = SparkSession.builder.appName(\"DataSourceExample\").getOrCreate()\n",
    "\n",
    "class DataSource:\n",
    "    def __init__(self, path):\n",
    "        self.path = path\n",
    "\n",
    "    def get_dataframe(self):\n",
    "        raise ValueError(\"Not implemented\")\n",
    "\n",
    "class CSVDataSource(DataSource):\n",
    "    def get_dataframe(self):\n",
    "        return spark.read.format(\"csv\").option(\"header\", True).load(self.path)\n",
    "\n",
    "class ParquetDataSource(DataSource):\n",
    "    def get_dataframe(self):\n",
    "        return spark.read.format(\"parquet\").load(self.path)\n",
    "\n",
    "class DeltaDataSource(DataSource):\n",
    "    def get_dataframe(self):\n",
    "        return spark.read.format(\"delta\").load(self.path)\n",
    "\n",
    "\n",
    "def get_data_source(data_type, file_path):\n",
    "    if data_type == \"csv\":\n",
    "        return CSVDataSource(file_path)\n",
    "    elif data_type == \"parquet\":\n",
    "        return ParquetDataSource(file_path)\n",
    "    elif data_type == \"delta\":\n",
    "        return DeltaDataSource(file_path)\n",
    "    else:\n",
    "        raise ValueError(f\"Not implemented for data_type: {data_type}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    data_source = get_data_source(\"csv\", \"dbfs:/FileStore/tables/Transaction_Updated.csv\")\n",
    "    df = data_source.get_dataframe()\n",
    "    df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13d8dcfd-b9f7-4580-ade5-eb48d8ac6515",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----------+------------+----------------+\n|transaction_id|customer_id|product_name|transaction_date|\n+--------------+-----------+------------+----------------+\n|            11|        105|      iPhone|      2022-02-01|\n|            12|        106|      iPhone|      2022-02-02|\n|            13|        107|     AirPods|      2022-02-03|\n|            14|        105|     AirPods|      2022-02-04|\n|            15|        108|      iPhone|      2022-02-05|\n|            16|        106|     MacBook|      2022-02-06|\n|            17|        107|      iPhone|      2022-02-07|\n|            18|        105|     MacBook|      2022-02-08|\n|            19|        108|     AirPods|      2022-02-09|\n|            20|        106|     AirPods|      2022-02-10|\n+--------------+-----------+------------+----------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Singleton SparkSession\n",
    "spark = SparkSession.builder.appName(\"DataSourceExample\").getOrCreate()\n",
    "\n",
    "class DataSource:\n",
    "    def __init__(self, path):\n",
    "        self.path = path\n",
    "\n",
    "    def get_dataframe(self):\n",
    "        \"\"\"Abstract method to be implemented by subclasses.\"\"\"\n",
    "        raise ValueError(\"Not implemented\")\n",
    "\n",
    "class CSVDataSource(DataSource):\n",
    "    def get_dataframe(self):\n",
    "        try:\n",
    "            return spark.read.format(\"csv\").option(\"header\", True).option(\"inferSchema\", True).load(self.path)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading CSV file {self.path}: {e}\")\n",
    "            return None\n",
    "\n",
    "class ParquetDataSource(DataSource):\n",
    "    def get_dataframe(self):\n",
    "        try:\n",
    "            return spark.read.format(\"parquet\").load(self.path)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading Parquet file {self.path}: {e}\")\n",
    "            return None\n",
    "\n",
    "class DeltaDataSource(DataSource):\n",
    "    def get_dataframe(self):\n",
    "        try:\n",
    "            return spark.read.format(\"delta\").load(self.path)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading Delta table {self.path}: {e}\")\n",
    "            return None\n",
    "\n",
    "def get_data_source(data_type, file_path):\n",
    "    \"\"\"Returns the appropriate data source class based on file type.\"\"\"\n",
    "    if data_type == \"csv\":\n",
    "        return CSVDataSource(file_path)\n",
    "    elif data_type == \"parquet\":\n",
    "        return ParquetDataSource(file_path)\n",
    "    elif data_type == \"delta\":\n",
    "        return DeltaDataSource(file_path)\n",
    "    else:\n",
    "        raise ValueError(f\"Not implemented for data_type: {data_type}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    file_path = \"dbfs:/FileStore/tables/Transaction_Updated.csv\"\n",
    "    data_source = get_data_source(\"csv\", file_path)\n",
    "    \n",
    "    df = data_source.get_dataframe()\n",
    "    \n",
    "    if df is not None:\n",
    "        df.show()\n",
    "    else:\n",
    "        print(\"Failed to load DataFrame.\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Reader_Factory",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
